---
layout: post
title: My Thoughts on AI (March 2025)
author: Edwin Choate
date: 2025-03-20
categories: ai
---

## First, let me set the context window

Let's get this out of the way... I'm writing this post on **March 20, 2025**. If you're going to be intellectually honest, you must judge everything I say in this post from the perpsective of March 20, 2025. 

## What's happening right now

As a refresher for readers from the future, here's what's currently happening: 

* Anthropic has released Sonnet 3.7 
* Sam Altman's most recent blog post on his website is [Three Observations](https://blog.samaltman.com/three-observations) wherein he doubled down on:
    * The claim that the way forward is to throw more compute at the problem
    * The claim that AI is about to get "10x" cheaper in the next "12 months"
    * The claim that, even _if_ we don't see exponential performance gains in the next 12 months, regular performance gains will lead to disproportionate benefits to customers.
    * The claim that "virtual agents" who "will eventually feel like virtual co-workers" are starting to be "rolled out". 
* DeepSeek-R1 has been released and open sourced
* Anthropic CEO Dario Amodei claims "we may be in a world" where AI will be "writing essentially all of the code" "in 12 months."
* AWS CEO Matt Garman states in an allegedly leaked internal meeting that "it's possible that most developers are not coding" in the near future (maybe "24 months" from now, or in "some amount of time").
* Mark Zuckerberg claims that "2025 will be the year it becomes possible to build an AI engineering agent that has coding and problem-solving abilities of around a good mid-level engineer." 
* Altman has started to talk about how AGI is closer than we think

## Look, this is complicated

Computers are complicated. Running a business is complicated. LLMs are complicated. Polictics are complicated. Customers are complicated. When you _combine_ all of these things together, man, it is hard to know what's going on. 

In many ways, this post is my (probably futile) attempt to do so. 

## Observation: everyone selling AI is using similar messaging

I find it interesting that "2025 is the year AI has exponential growth". How is it that all of the companies are concluding this on their own accord? Are the Meta engineers at the bleeding edge of AI reporting to Zuckerberg an estimate of when they expect to finish automating their peers' jobs? Is Mark diving into using these AI tools himself to gauge the progress being made?

I find it more plausable that, instead of making these claims based on what it actually happening with the technology, the claims that are being made are the ones that best serve the companies and their upcoming goals. "2025" represents Q1 - Q4 goals. And it also represents how much funding they've secured. 

It's pretty easy to tell how much longer Microsoft is willing to shell out the billions of dollars in cash that it takes to keep this party going: you can just ask Microsoft how long they are willing to keep paying. 

"In two years" might mean, "we know we've got two years of funding to figure this out." 

## A spoon full of hype makes the investment go down

Remember the Metaverse? 

Remember self-driving cars?

We didn't end up getting any of those things. But we _did_ get VR headsets and auto-pilot. 

As an aside, I think it's a bit of a shame that we don't get to revel in how awesome VR or auto-pilot is. Those are legitmately futuristic innovations that we've successfully built. We don't take the time we should to appreciate how cool they are. (And we don't do this, because the expectation that was set for them was bigger: auto-pilot is underwhelming compared to an autonomous vehicle.) 

If I'm an investor and you're asking for a bazillion dollars to build some VR headsets, I might not be able to justify that expense. But, if you're telling me that you're building a dystopian virtual society where everyone will live in the future and whoever doesn't get on board now will be left behind, then it'd be a risk _not_ to invest in that, right? Right? 

Look, I think Oculus is a really cool product. I have no objections to the idea of a well-made VR headset. What I do object to is not calling a spade a spade. 

Unfortunately, overhyping your business to get VC funding is a necessary evil in emerging markets - a thing you have to do if you want the chance to participate. 

## The logical fallacy of replacing human software developers with LLMs

There's a problem with the suggestion that you could replace a software developer with an AI developer in "the next few years". 

It goes something like this... 

In order for it to be true that you're replacing the developer, you must get rid of that person. 

If that person is gone, then whoever is writing the plain-English prompt that asks the AI to write all of the code needs to be able to do all of the work by writing the prompts in natural language (English, not code). They cannot look at the code. (If they're looking at the code, then that means they are a coder: someone who knows how to read and write code.) 

If they're not looking at the code, then, think about it... that would be analogous to doing something like this:

Ask ChatGPT to compose an email to your boss. Copy what it writes. Don't read any of the content of the email. Paste the email into your email client and hit send. 

You mean to tell me that "in the next few years", companies will be willing to take a risk like that, rolling the dice that the software running their businesses is just coded however the AI decides it should be coded? That's like copy/pasting a ChatGPT thread and sending it to your boss without looking at it, _every_ time you write an email. 

Business leaders are really going to be willing to stake their reputations on that? 

## A much more likely scenario

To continue with the email analogy above, let's say you _did_ review the contents of the email ChatGPT wrote before sending it to your boss. That would mitigate the risk almost entirely. You'd be able to instantly tell if ChatGPT was saying something that you didn't want it to say. It would be trivially easy to identify those things. 

You'd be able to do this because you can read English, the same language ChatGPT used to compose the email. 

I think the same will happen with code. 

Some "coder" (a person who knows how to read and write code) can use something like Claude Code 3.7 Sonnet (or, the new & improved version of it) to generate a lot of code, and then they can take some action: approve a small code snippet, make one choice within a set of options, provide context, etc. 

This scenario does not contain the same logical fallacy that the above scenario contains. 

Therefore, to continue to run your business, you need a coder (someone who can read and write code) in the loop.

## So, will AI take junior developers' jobs? 

At the time is this writing, there are a _lot_ of people online speculating that AI will take junior developer jobs. 

This seems plausible at first glance, but it too has issues. 

If the junior developer job goes away, then who will become the senior developers in the future? Every day, there are senior developers who are becoming managers, exiting the field, or retiring. Who's going to replace them? 

Not having the job of junior developer is like trying to run a Major League Baseball team without having any Minor League affiliates. 

I think the point of the junior developer job is to put yourself out of a job by learning and turning yourself into a senior developer. 

Every field, no matter the subject matter, has the same problem: why hire young people who have relatively little skill? Come to think of it, this might be a timeless problem. If you don't hire the next generation to get busy learning how to be useful, then society stops functioning eventually.

So in a world where there are no junior developers, and all the senior developers age out, who's left? Is all of society running on software that we can no longer edit? That seems untenable. 

It seems more likely to me that companies will always need to employ techincally-minded people who have whatever technical expertise is necessary to keep the systems from falling apart.

## What do we do with the theoretical efficiency gains? 

It's plausible to me that with speed & efficiency gains developers get from AI, that means businesses can lay off some of the developers on the staff. 

It's also plausible to me that it will just up the competition between businesses. 

If my competitor lays off half of their developers and continues the same output as before, but I keep all my developers and double my output, do I win? 

I think the most realistic scenario is a little of both: there is some reduction in force (you get rid of the least impactful employees) and there is some increase in productivity from the new tools.

## As technology advances, we get new languages 

Like I said in the above section titled _Look, this is complicated_, this is in fact complicated. One possible future I see is that programming itself changes in some difficult-to-predict way. 

There is _plenty_ of historical precidence for this. We don't have many assembly programmers anymore, few people still write COBOL for a living, and gone are the days of the FORTRAN programmer. Even C++ and Java programming is becoming a bit of a lost art.

This is the change that is the constant. 

As there are breakthroughs in technologies, the programming languages change.

A programmer is, in a sense, a person who knows how to communicate to a computer to tell it what to do. Clearly, LLMs are a game-changer when it comes to communicating with computers. 

## We think we have a game-changer in LLMs, but it isn't yet clear how the game will be changed 

Up till now, all of the programming languages that have been written are designed for human writers. 

One day, in the future, will there be programming languages that are designed for AI writers + human readers/editors? I find this to be a fascinating question.

There's been talk that "the hottest new programming language is English". I have my doubts about that. English is a language that was designed for humans to communicate with humans. It seems unlikely to me that it just so happens to be the best way for humans to communicate with LLM-equipped computers. 

I took a logic class in college. We represented the logic with Greek symbols. Similarly, in mathematics you use symbols to teach children (who already know English) mathematical concepts. 

I think there maybe be some timeless truth at play here, that there's no better way to represent logic than using symbols. That might factor into the new language design choices we make in the future.

## Language and meaning are different things

Say you've got a tool that allows you to have a perfectly fluent conversation with a computer in a natural language that is so intuitive that you don't have to spend any time learning said language. Say that tool is as good as it possible can be. In other words, the language problems are all 100% solved. What still remains is the content of the conversation and what it means towards the aim of productivity.

What would you two talk about, exactly? The weather? 

If you have no idea what is going on under-the-hood, then you have to trust all of the computer's decisions are good ones. 

Think about that... the computer would be making decisions on your behalf. These would be decisions that you would not supervise or even understand, because you don't know what is going on inside the computer. 

To me, this seems utterly ineffective compared to an alternative: a human being who knows what's going on inside the computer and has the ability to have a conversation with the computer about the decisions (especially the important decisions). 

## What is the source of the doom messaging? 

I want to explicitly include this section in this article, because (if my theory is correct), I don't want to let the tech leaders off the hook. 

I think that the primary reason influencers share doom-and-gloom "AI will take our jobs" content on social right now is that it is getting a lot of attention. 

I think some of the influencers are being paid to put together demos of the AI models to make them seem better than they really are, and those posts are being intentionally amplified by the AI companies. 

And, I think there are sincere software developers who are trying the stuff out and sharing honestly about how far it has to go. 

The one question I'd pose here is: where are the thousands of videos of people using AI to build software with ease? 

## I tried out Copilot, and it sucked

Let's just say that Copilot has a long way to go. The most rediculous thing it's done for me so far is it put this file (that it clearly ripped off from some article) into my project: `path\to\your\project.csproj`. It literally put that file in my project.

This could easy be because the tech isn't ready yet. But, I want to point out that the AI tech leaders are all claiming that coders will be replaced in one calendar year. These two things, at the present time, do not square at all.

Before you tell me to try 3.7 Sonnet, I do intend to do that soon, but I want to point out that Copilot is Microsoft's product, the same company that has supposedly partnered with OpenAI to create something revolutionary.

## So, if the tech leaders are lying, why? 

This might be the most important question to think about. Refer to the earlier section about hyping up investors. There's a whole lot of that happening. 

I also think there's an additional factor: these products (such as ChatGPT) are extremely expensive to operate and these tech leaders can sense that the investors are losing some patience. 

If it's true that they're lying, then they're likely doing it for the same reason that an email scammer creates false urgency: triggering fear numbs calculated thinking. If investors are afraid they might miss out, they'll be more likely to fork over the cash, even when it might not be a great investment right now. 

I'm personally suspicious that all this doom and gloom is a smokescreen.

When you write a post titled _Three Observations_ that transparently contains three _claims_, it's fairly self-evident that you are trying to prop up something that isn't necessarily agreed-upon fact. That's worthy of some suspicion in and of itself. Why not share the amazing progress that is supposedly happening? Wouldn't that be better content? 

A lot of this reminds me of when we were going to get "self-driving cars" within 2 years... and that was said well more than 2 years ago. At the end of the day, that particular lie got Telsa the auto-pilot feature it needed to pull ahead in the market. So, in a sense, it worked. 

## Meanwhile, is China playing with us? 

I think the DeepSeek products are important to pay attention at the very least because they represent a cold, hard fact: China has figured out how to make LLMs orders of magnitudes cheaper to run and OpenAI hasn't figured that out yet. I think this is the crux of the US stock market panic that followed DeepSeek's release.

You could interpret this as China toying with us, open sourcing something we can't do, just to send a message of their dominence in the engineering sector. 

Another way to read this would be that making things that the US has innovated, copying that, and making it cheaper is the thing that China is good at. What's remarkable perhaps about this is that you've got a scenario where China has made a product cheaper _before_ the US has figured out a self-sustaining, profitable market fit.

## So, what's going to happen? 

It's very difficult to predict the future, and that seems to be a timeless truth. It always looks so obvious in hindsight. At the risk of eating my words, I am having fun taking a stab at it. I look forward to looking back at this one day, and seeing what I got right and what I didn't. 

Here goes... 

* The ChatGPT/LLM hype will die down as we the people, who have remarkably short attention spans, get bored with it
* The big tech companies will eventually, as the hype dies down, no longer be able to justify pouring in billions of dollars into these products. This will force the tech companies to make difficult decisions, which they'll figure out. They're smart. 
* LLMs will be used to power a variety of savvy software features, from sophisticated search to smart in-product help documentation.
* The "agents" that supposedly are coming soon to take the white collar computer jobs will not actually take the white collar computer jobs. White collar workers will continue doing white collar work.
* China will start leading when it comes to LLMs
* A smart auto-complete / code generation featureset will be powered by LLMs and will be refined and put into all the major IDEs. Developers will use these in their work to varying degrees, depending on the problem at hand. It will not replace developers.
* Productivity in developing software will increase (but it won't skyrocket) 
* There will be famous headlines of security breaches, infamous bugs, and other issues attributed to LLMs writing code and humans carelessly approving it. 
* Junior developers will continue to exist. It will _continue_ to be difficult but not impossible for them to get entry-level jobs. 
* The title of "software developer" will change to something else. There will continue to be work for technical-minded people who are responsible for understanding technical systems, adding new features, and making sure the systems don't break down. 
* There will continue to be a full spectrum of salaries for software developers (or whatever we call them). Those working on complex large-scale critical infrustructure will continue to be extremely highly-paid. Others doing work that doesn't require as much skill will continue to be paid less. 
* The skill of being fast at manually typing out code will plummet in importance
* We will continue to have human artists, writers, and programmers
* Taste and curation will become a differentiator in the market
* The aesthetic of the "human touch" will become in-demand amongst people with wealth, and it will become a status symbol. "Robotic" things will be viewed as cheap and low-class.

And lastly, I predict: 

* We will be okay

